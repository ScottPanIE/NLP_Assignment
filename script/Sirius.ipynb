{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize, WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import PolynomialFeatures,OneHotEncoder,LabelEncoder,StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/fake_or_real_news_training.csv')\n",
    "# the submission data has no label\n",
    "submission = pd.read_csv('../data/fake_or_real_news_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "# define function to eliminate the stopwords\n",
    "def eliminate_stopwords(wordslist):\n",
    "    \"\"\"\n",
    "    stopwords_en is predefined outside of the function\n",
    "    \"\"\"\n",
    "    wordslist = [i for i in wordslist if i.isalpha()]\n",
    "    clean_list = [i for i in wordslist if i not in stopwords_en]\n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postag\n",
    "def count_postags(words):\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    num_noun = 0\n",
    "    num_verb = 0\n",
    "    num_adj = 0\n",
    "    for word in tagged_words:\n",
    "        if word[1] == 'NN':\n",
    "            num_noun += 1\n",
    "        elif word[1] == 'VERB':\n",
    "            num_verb += 1\n",
    "        elif word[1] == 'ADJ':\n",
    "            num_adj += 1\n",
    "    return num_noun, num_verb, num_adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect keywords\n",
    "def detect_keyword(words, keyword):\n",
    "    if keyword in words:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the training set has 33 rows of data in the wrong format\n",
    "# we are going to simply remove the rows\n",
    "print(len(train.label.unique()))\n",
    "train = train[(train.label == 'REAL')|(train.label == 'FAKE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    \"\"\"\n",
    "    preprocessing + feature engineering\n",
    "    \"\"\"\n",
    "    # drop the redundent features\n",
    "    data = data.drop(['X1','X2'],axis=1)\n",
    "    # tokenize the title and text\n",
    "    data['title_token'] = data.title.apply(lambda x:word_tokenize(x.lower()))\n",
    "    data['text_token'] = data.text.apply(lambda x:word_tokenize(x.lower()))\n",
    "    # convert the target variable to 0 and 1\n",
    "    data.label = data.label.apply(lambda x:1 if x == 'REAL' else 0)\n",
    "    \n",
    "    # eliminate the stopwords in title and text\n",
    "    data['titletoken_without_stopwords'] = data.title_token.apply(lambda x:eliminate_stopwords(x))\n",
    "    data['texttoken_without_stopwords'] = data.text_token.apply(lambda x:eliminate_stopwords(x))\n",
    "    # drop the redundent features\n",
    "    data = data.drop(['text','title'],axis=1)\n",
    "    # need to eliminate the punctuation as well? \n",
    "    # maybe do it with regex from the original title and text\n",
    "\n",
    "    ## feature engineering \n",
    "    \n",
    "    # find keywords\n",
    "    data['trump_title'] = data.titletoken_without_stopwords.apply(lambda x:detect_keyword(x,'trump'))\n",
    "    data['trump_text'] = data.texttoken_without_stopwords.apply(lambda x:detect_keyword(x,'trump'))\n",
    "    \n",
    "    # count the postags\n",
    "    # title\n",
    "    data['title_postags'] = data.titletoken_without_stopwords.apply(lambda x:count_postags(x))\n",
    "    data['title_NN_count'] = data.title_postags.map(lambda x:x[0])\n",
    "    data['title_VERB_count'] = data.title_postags.map(lambda x:x[1])\n",
    "    data['title_ADJ_count'] = data.title_postags.map(lambda x:x[2])\n",
    "    # text\n",
    "    data['text_postags'] = data.texttoken_without_stopwords.apply(lambda x:count_postags(x))\n",
    "    data['text_NN_count'] = data.text_postags.map(lambda x:x[0])\n",
    "    data['text_VERB_count'] = data.text_postags.map(lambda x:x[1])\n",
    "    data['text_ADJ_count'] = data.text_postags.map(lambda x:x[2])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # create new features describe the length of the title and text, counting by words\n",
    "    # also maybe try with counting by letters\n",
    "    # and shall treat them as categorical variables group up with factor levels \n",
    "    data['title_length'] = data.title_token.apply(lambda x:len(x))\n",
    "    data['text_length'] = data.text_token.apply(lambda x:len(x))\n",
    "    # also highlight the keywords maybe?\n",
    "    \n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = prepare_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitdata(data, cols):\n",
    "    \"\"\"\n",
    "    cols: a list of columns names to use\n",
    "    \"\"\"\n",
    "    features = data[cols]\n",
    "    target = data.label\n",
    "    # first split into (train+test) and for holdout\n",
    "    TrainingSetX, X_holdout, TrainingSetY, y_holdout = train_test_split(features, target, \n",
    "                                                    test_size=0.10, stratify=target, \n",
    "                                                    random_state=666)\n",
    "    # split (train+test) into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(TrainingSetX, TrainingSetY, \n",
    "                                                test_size=0.20, stratify=TrainingSetY, \n",
    "                                                random_state=666)\n",
    "    # train and test use for modeling, holdout use for validating the model\n",
    "    print(\"spliting the data......\")\n",
    "    print(\"shape of train set: \", X_train.shape)\n",
    "    print(\"shape of test set: \", X_test.shape)\n",
    "    print(\"shape of holdout set: \", X_holdout.shape)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, X_holdout, y_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliting the data......\n",
      "shape of train set:  (2879, 5)\n",
      "shape of test set:  (720, 5)\n",
      "shape of holdout set:  (400, 5)\n"
     ]
    }
   ],
   "source": [
    "cols2model = ['title_token', 'text_token', 'titletoken_without_stopwords', \n",
    "              'texttoken_without_stopwords', 'title_length']\n",
    "x1,y1,x2,y2,x3,y3 = splitdata(train_df, cols = cols2model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest classifier\n",
    "def model_rf(data,cols2model):\n",
    "    # split data\n",
    "    X_train, y_train, X_test, y_test, X_holdout, y_holdout = splitdata(train_df, cols = cols2model)\n",
    "    print('features for modeling :',X_train.columns.tolist())\n",
    "    # identify the categorical features and the numerical features\n",
    "    categorical_features = X_train.dtypes == 'category'\n",
    "    numerical_features = ~categorical_features\n",
    "    # pipeline\n",
    "    print('building up pipeline......')\n",
    "    estimators = [   \n",
    "                    ('ctf',ColumnTransformer([\n",
    "                                ('scale',StandardScaler(),\n",
    "                                         numerical_features),\n",
    "                                ('enc',OneHotEncoder(categories='auto',handle_unknown='ignore'),\n",
    "                                         categorical_features),\n",
    "                                            ])\n",
    "                    ),\n",
    "#                     ('poly', PolynomialFeatures(degree=1, include_bias=False, interaction_only=True)),\n",
    "                    ('RF',RandomForestClassifier(random_state=666,n_estimators=170,max_depth=22,n_jobs=-1))\n",
    "                ]\n",
    "    pipe = Pipeline(estimators)\n",
    "    \n",
    "    print('training the model......')\n",
    "    model = pipe.fit(X_train,y_train)\n",
    "    \n",
    "    print('training is done')\n",
    "    score_train = model.score(X_train,y_train) \n",
    "    score_test = model.score(X_test,y_test) \n",
    "    score_Holdout = model.score(X_holdout,y_holdout) \n",
    "    \n",
    "    print('training dataset mode1 is: %s'%(str(score_train1)))   \n",
    "    print('test dataset model is: %s'%(str(score_test1)))\n",
    "    print('Holdout dataset model1 is: %s'%(str(score_H1)))\n",
    "    \n",
    "    # in case needs to use the data to do some visualization or some other metrics\n",
    "    data_pack = (X_train, y_train, X_test, y_test, X_holdout, y_holdout)\n",
    "    return model, data_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sentence', '.']\n",
      "this is a sentence.\n"
     ]
    }
   ],
   "source": [
    "try_text = 'this is a Sentence.'\n",
    "result = word_tokenize(try_text.lower())\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['text_try'] = sample.text.apply(lambda x:word_tokenize(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
