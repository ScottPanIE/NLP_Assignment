{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk import word_tokenize, WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/fake_or_real_news_training.csv')\n",
    "# the submission data has no label\n",
    "submission = pd.read_csv('../data/fake_or_real_news_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First look at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0   8476                       You Can Smell Hillary’s Fear   \n",
       "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label   X1   X2  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  NaN  NaN  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  NaN  NaN  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  NaN  NaN  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  NaN  NaN  \n",
       "4  It's primary day in New York and front-runners...  REAL  NaN  NaN  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that two extra columns were created, X1 and X2. This is due to the fact that the title is not in quotes such that, if there is a comma in the title, it get's separated at that point and all the other column get shifted. The following cell will take care of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange(df):\n",
    "    for row in range(len(df)):\n",
    "        if not pd.isna(df.iloc[row, 5]):\n",
    "            # Concatenate according columns\n",
    "            df.iloc[row, 1] = df.iloc[row, 1] + df.iloc[row, 2] + df.iloc[row, 3]\n",
    "            df.iloc[row, 2] = df.iloc[row, 4]\n",
    "            df.iloc[row, 3] = df.iloc[row, 5]\n",
    "        elif not pd.isna(df.iloc[row, 4]):\n",
    "            df.iloc[row, 1] = df.iloc[row, 1] + df.iloc[row, 2]\n",
    "            df.iloc[row, 2] = df.iloc[row, 3]\n",
    "            df.iloc[row, 3] = df.iloc[row, 4]\n",
    "    df = df.drop(['X1', 'X2'], axis = 1)\n",
    "    return df\n",
    "\n",
    "train = rearrange(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0   8476                       You Can Smell Hillary’s Fear   \n",
       "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proportion of fake-news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4978744686171543"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.loc[train['label'] == 'FAKE'])/len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore phrase length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a20853550>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_len = pd.DataFrame([len(t) for t in train.text])\n",
    "sns.distplot(text_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is highly skewed to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a baseline without further preprocessing using the countvectorizer, using the text only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_baseline, X_test_baseline, y_baseline, y_test_baseline = train_test_split(train['text'], train['label'],\n",
    "                                                                            test_size = 0.2, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count-vectorize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiaulize countvectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform \n",
    "count_train = count_vectorizer.fit_transform(X_baseline) \n",
    "\n",
    "# Transform the test set \n",
    "count_test = count_vectorizer.transform(X_test_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Naive Bayes for a first classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MltNB = MultinomialNB() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[348,  57],\n",
       "       [ 34, 361]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MltNB.fit(count_train, y_baseline)\n",
    "pred = MltNB.predict(count_test)\n",
    "acc = accuracy_score(y_test_baseline, pred)\n",
    "print(\"accuracy:   %0.3f\" % acc)\n",
    "\n",
    "confusion_matrix(y_test_baseline, pred, labels=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Model\n",
    "1. Different Model\n",
    "2. More Process on Data\n",
    "3. Tuning Parameters\n",
    "4. Additional thoughts: can we add another model to predicit base on news' title, then ensemble both models to make a final prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can tune max_df later\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8, norm='l2')\n",
    "# transform data\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_baseline)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[347,  58],\n",
       "       [ 19, 376]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MltNB_TFIDF = MultinomialNB(alpha=0.1) \n",
    "MltNB_TFIDF.fit(tfidf_train, y_baseline)\n",
    "pred_TFIDF = MltNB_TFIDF.predict(tfidf_test)\n",
    "acc_TFIDF = accuracy_score(y_test_baseline, pred_TFIDF)\n",
    "print(\"accuracy:   %0.3f\" % acc_TFIDF)\n",
    "\n",
    "confusion_matrix(y_test_baseline, pred_TFIDF, labels=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    \"\"\"\n",
    "    processing data, on text and title\n",
    "    \"\"\"\n",
    "    # fix X1 X2 issue\n",
    "    data = rearrange(data)\n",
    "    # tokenize the title and text\n",
    "    data['title'] = data.title.apply(lambda x:\" \".join(word_tokenize(x.lower())))\n",
    "    data['text'] = data.text.apply(lambda x:\" \".join(word_tokenize(x.lower())))\n",
    "    # convert the target variable to 0 and 1\n",
    "    data.label = data.label.apply(lambda x:1 if x == 'REAL' else 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload data\n",
    "train_0 = pd.read_csv('../data/fake_or_real_news_training.csv')\n",
    "train_1 = prepare_data(train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "def split_data(data, feature='text'):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[feature], data['label'],\n",
    "                                                        test_size = 0.2, random_state = 7)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize\n",
    "def vectorize_select(selection=\"tfidf\", max_df=0.8):\n",
    "    \"\"\"\n",
    "    \"tfidf\":\"TfidVectorizer\"\n",
    "    \"count\":\"CountVectorizer\"\n",
    "    \"hash\":\"HashingVectorizer\"\n",
    "    \"\"\"\n",
    "    if selection == \"tfidf\":\n",
    "        return TfidfVectorizer(stop_words='english', max_df=max_df)\n",
    "    elif selection == \"count\":\n",
    "        return CountVectorizer(stop_words='english', max_df=max_df)\n",
    "    elif selection == \"hash\":\n",
    "        return HashingVectorizer(stop_words='english')\n",
    "    else:\n",
    "        raise Exception(\"{} can't be found\".format(selection))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ti-idf vectorize MultinomialNB model\n",
    "def nb_model(data,selection='tfidf',vectorize_max_df=0.8):\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = split_data(data)\n",
    "    \n",
    "    # vectorizer: selection: 'tfidf','count','hash'\n",
    "    vectorizer = vectorize_select(selection, max_df=vectorize_max_df)\n",
    "    # transform data\n",
    "    vectorize_train = vectorizer.fit_transform(X_train)\n",
    "    vectorize_test = vectorizer.transform(X_test)\n",
    "\n",
    "    # model \n",
    "    MltNB = MultinomialNB(alpha=0.1) \n",
    "    MltNB.fit(tfidf_train, y_train)\n",
    "    pred = MltNB.predict(tfidf_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % acc)\n",
    "    print(confusion_matrix(y_test, pred, labels=[0, 1]))\n",
    "    \n",
    "    return MltNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.904\n",
      "[[347  58]\n",
      " [ 19 376]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model(train_1,'tfidf',0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.904\n",
      "[[347  58]\n",
      " [ 19 376]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model(train_1,'hash',0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.904\n",
      "[[347  58]\n",
      " [ 19 376]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model(train_1,'count',0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ti-idf vectorize rf model\n",
    "def rf_model(data,selection='tfidf',vectorize_max_df=0.8):\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = split_data(data)\n",
    "    \n",
    "    # vectorizer: selection: 'tfidf','count','hash'\n",
    "    vectorizer = vectorize_select(selection, max_df=vectorize_max_df)\n",
    "    # transform data\n",
    "    vectorize_train = vectorizer.fit_transform(X_train)\n",
    "    vectorize_test = vectorizer.transform(X_test)\n",
    "    \n",
    "    # pipeline\n",
    "    estimators = [('RF',RandomForestClassifier(random_state=666,n_estimators=100,max_depth=20))]\n",
    "    pipe = Pipeline(estimators)\n",
    "    param_RF = {'RF__max_depth': [20,25],\n",
    "                'RF__n_estimators': [100,150]}\n",
    "    \n",
    "    # apply the estimators and parameters in pipeline\n",
    "    gridPipe = GridSearchCV(pipe, param_RF, cv=5, return_train_score=True)\n",
    "    model = gridPipe.fit(vectorize_train, y_train)\n",
    "\n",
    "    pred = model.predict(vectorize_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % acc)\n",
    "    print(confusion_matrix(y_test, pred, labels=[0, 1]))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.916\n",
      "[[380  25]\n",
      " [ 42 353]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('RF', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=666, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'RF__max_depth': [20, 25], 'RF__n_estimators': [100, 150]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model(train_1,'tfidf',0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.881\n",
      "[[369  36]\n",
      " [ 59 336]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('RF', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=666, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'RF__max_depth': [20, 25], 'RF__n_estimators': [100, 150]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model(train_1,'hash',0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.901\n",
      "[[373  32]\n",
      " [ 47 348]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('RF', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=666, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'RF__max_depth': [20, 25], 'RF__n_estimators': [100, 150]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model(train_1,'count',0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PAC(data, selection='tfidf', vectorize_max_df=0.8, feature='text'):\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = split_data(data, feature=feature)\n",
    "    # vectorizer: selection: 'tfidf','count','hash'\n",
    "    vectorizer = vectorize_select(selection, max_df=vectorize_max_df)\n",
    "    # transform data\n",
    "    vectorize_train = vectorizer.fit_transform(X_train)\n",
    "    vectorize_test = vectorizer.transform(X_test)\n",
    "    # model \n",
    "    linear_clf = PassiveAggressiveClassifier(random_state=666, max_iter=100, tol=1e-3,\n",
    "                                             early_stopping=True, validation_fraction=0.1)\n",
    "    linear_clf.fit(vectorize_train, y_train)\n",
    "    pred = linear_clf.predict(vectorize_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % acc)\n",
    "    print(confusion_matrix(y_test, pred, labels=[0, 1]))\n",
    "    \n",
    "    return linear_clf, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.932\n",
      "[[379  26]\n",
      " [ 28 367]]\n"
     ]
    }
   ],
   "source": [
    "m_text, v_text = PAC(train_1,'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.909\n",
      "[[365  40]\n",
      " [ 33 362]]\n"
     ]
    }
   ],
   "source": [
    "m = PAC(train_1,'hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.869\n",
      "[[346  59]\n",
      " [ 46 349]]\n"
     ]
    }
   ],
   "source": [
    "m = PAC(train_1,'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model on Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.787\n",
      "[[320  85]\n",
      " [ 85 310]]\n"
     ]
    }
   ],
   "source": [
    "m_title, v_title = PAC(train_1, 'tfidf', feature='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.776\n",
      "[[308  97]\n",
      " [ 82 313]]\n"
     ]
    }
   ],
   "source": [
    "m = PAC(train_1, 'count', feature='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.784\n",
      "[[316  89]\n",
      " [ 84 311]]\n"
     ]
    }
   ],
   "source": [
    "m = PAC(train_1, 'hash', feature='title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. predict base on text (train on training set, predict on the entire)\n",
    "2. predict base on title (train on training set, predict on the entire)\n",
    "3. add two predictions to the dataframe\n",
    "4. maybe concatenate title and text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding text based prediction to data\n",
    "data_text = v_text.transform(train_1.text)\n",
    "data_text_prediction = m_text.predict(data_text)\n",
    "train_1['text_predict'] = data_text_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding title based prediction to data\n",
    "data_title = v_title.transform(train_1.title)\n",
    "data_title_prediction = m_title.predict(data_title)\n",
    "train_1['title_predict'] = data_title_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "# define function to eliminate the stopwords\n",
    "def eliminate_stopwords(wordslist):\n",
    "    \"\"\"\n",
    "    stopwords_en is predefined outside of the function\n",
    "    \"\"\"\n",
    "    wordslist = [i for i in wordslist if i.isalpha()]\n",
    "    clean_list = [i for i in wordslist if i not in stopwords_en]\n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postag\n",
    "def count_postags(words):\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    num_noun = 0\n",
    "    num_verb = 0\n",
    "    num_adj = 0\n",
    "    for word in tagged_words:\n",
    "        if word[1] == 'NN':\n",
    "            num_noun += 1\n",
    "        elif word[1] == 'VERB':\n",
    "            num_verb += 1\n",
    "        elif word[1] == 'ADJ':\n",
    "            num_adj += 1\n",
    "    return num_noun, num_verb, num_adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect keywords\n",
    "def detect_keyword(words, keyword):\n",
    "    if keyword in words:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need one more time process to create additional features\n",
    "def prepare_data2(data):\n",
    "    \"\"\"\n",
    "    feature engineering\n",
    "    \"\"\"\n",
    "    # tokenize the title and text\n",
    "    data['title_token'] = data.title.apply(lambda x:word_tokenize(x.lower()))\n",
    "    data['text_token'] = data.text.apply(lambda x:word_tokenize(x.lower()))\n",
    "\n",
    "    # eliminate the stopwords in title and text\n",
    "    data['titletoken_without_stopwords'] = data.title_token.apply(lambda x:eliminate_stopwords(x))\n",
    "    data['texttoken_without_stopwords'] = data.text_token.apply(lambda x:eliminate_stopwords(x))\n",
    "\n",
    "    # need to eliminate the punctuation as well? \n",
    "    # maybe do it with regex from the original title and text\n",
    "\n",
    "    ## feature engineering \n",
    "    \n",
    "    # find keywords\n",
    "    data['trump_title'] = data.titletoken_without_stopwords.apply(lambda x:detect_keyword(x,'trump'))\n",
    "    data['trump_text'] = data.texttoken_without_stopwords.apply(lambda x:detect_keyword(x,'trump'))\n",
    "    \n",
    "    # count the postags\n",
    "    # title\n",
    "    data['title_postags'] = data.titletoken_without_stopwords.apply(lambda x:count_postags(x))\n",
    "    data['title_NN_count'] = data.title_postags.map(lambda x:x[0])\n",
    "    data['title_VERB_count'] = data.title_postags.map(lambda x:x[1])\n",
    "    data['title_ADJ_count'] = data.title_postags.map(lambda x:x[2])\n",
    "    # text\n",
    "    data['text_postags'] = data.texttoken_without_stopwords.apply(lambda x:count_postags(x))\n",
    "    data['text_NN_count'] = data.text_postags.map(lambda x:x[0])\n",
    "    data['text_VERB_count'] = data.text_postags.map(lambda x:x[1])\n",
    "    data['text_ADJ_count'] = data.text_postags.map(lambda x:x[2])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # create new features describe the length of the title and text, counting by words\n",
    "    # also maybe try with counting by letters\n",
    "    # and shall treat them as categorical variables group up with factor levels \n",
    "    data['title_length'] = data.title_token.apply(lambda x:len(x))\n",
    "    data['text_length'] = data.text_token.apply(lambda x:len(x))\n",
    "    # also highlight the keywords maybe?\n",
    "    \n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text_predict</th>\n",
       "      <th>title_predict</th>\n",
       "      <th>trump_title</th>\n",
       "      <th>trump_text</th>\n",
       "      <th>title_NN_count</th>\n",
       "      <th>title_VERB_count</th>\n",
       "      <th>title_ADJ_count</th>\n",
       "      <th>text_NN_count</th>\n",
       "      <th>text_VERB_count</th>\n",
       "      <th>text_ADJ_count</th>\n",
       "      <th>title_length</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>253</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  text_predict  title_predict  trump_title  trump_text  \\\n",
       "0      0             0              0            0           1   \n",
       "1      0             0              0            1           1   \n",
       "2      1             1              1            0           0   \n",
       "3      0             0              0            0           1   \n",
       "4      1             1              1            0           1   \n",
       "\n",
       "   title_NN_count  title_VERB_count  title_ADJ_count  text_NN_count  \\\n",
       "0               2                 0                0            253   \n",
       "1               8                 0                0             93   \n",
       "2               3                 0                0             85   \n",
       "3               4                 0                0             78   \n",
       "4               2                 0                0             54   \n",
       "\n",
       "   text_VERB_count  text_ADJ_count  title_length  text_length  \n",
       "0                0               0             7         1473  \n",
       "1                0               0            16          516  \n",
       "2                0               0             9          485  \n",
       "3                0               0            18          497  \n",
       "4                0               0            10          376  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_2 = prepare_data2(train_1)\n",
    "train_3 = train_2[['label','text_predict','title_predict','trump_title', 'trump_text',\n",
    "                   'title_NN_count', 'title_VERB_count','title_ADJ_count', \n",
    "                   'text_NN_count', 'text_VERB_count','text_ADJ_count', \n",
    "                   'title_length', 'text_length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ti-idf vectorize rf model\n",
    "def rf_model_2(data,features=features):\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = split_data(data,feature=features)\n",
    "    \n",
    "    # pipeline\n",
    "    estimators = [('RF',RandomForestClassifier(random_state=666))]\n",
    "    pipe = Pipeline(estimators)\n",
    "    param_RF = {'RF__max_depth': [20,25],\n",
    "                'RF__n_estimators': [150,200]}\n",
    "    \n",
    "    # apply the estimators and parameters in pipeline\n",
    "    gridPipe = GridSearchCV(pipe, param_RF, cv=5, return_train_score=True)\n",
    "    model = gridPipe.fit(X_train, y_train)\n",
    "\n",
    "    pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % acc)\n",
    "    print(confusion_matrix(y_test, pred, labels=[0, 1]))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.932\n",
      "[[379  26]\n",
      " [ 28 367]]\n"
     ]
    }
   ],
   "source": [
    "features = ['text_predict','title_predict',\n",
    "            'trump_title', 'trump_text',\n",
    "#             'title_NN_count', 'title_VERB_count','title_ADJ_count', \n",
    "#             'text_NN_count', 'text_VERB_count','text_ADJ_count', \n",
    "#             'title_length', 'text_length'\n",
    "           ]\n",
    "m = rf_model_2(train_3,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    \"\"\"\n",
    "    preprocessing + feature engineering\n",
    "    \"\"\"\n",
    "    # tokenize the title and text\n",
    "    data['title_token'] = data.title.apply(lambda x:word_tokenize(x.lower()))\n",
    "    data['text_token'] = data.text.apply(lambda x:word_tokenize(x.lower()))\n",
    "    # convert the target variable to 0 and 1\n",
    "    data.label = data.label.apply(lambda x:1 if x == 'REAL' else 0)\n",
    "    \n",
    "    # eliminate the stopwords in title and text\n",
    "    data['titletoken_without_stopwords'] = data.title_token.apply(lambda x:eliminate_stopwords(x))\n",
    "    data['texttoken_without_stopwords'] = data.text_token.apply(lambda x:eliminate_stopwords(x))\n",
    "    # drop the redundent features\n",
    "    data = data.drop(['text','title'],axis=1)\n",
    "    # need to eliminate the punctuation as well? \n",
    "    # maybe do it with regex from the original title and text\n",
    "\n",
    "    ## feature engineering \n",
    "    \n",
    "    # find keywords\n",
    "    data['trump_title'] = data.titletoken_without_stopwords.apply(lambda x:detect_keyword(x,'trump'))\n",
    "    data['trump_text'] = data.texttoken_without_stopwords.apply(lambda x:detect_keyword(x,'trump'))\n",
    "    \n",
    "    # count the postags\n",
    "    # title\n",
    "    data['title_postags'] = data.titletoken_without_stopwords.apply(lambda x:count_postags(x))\n",
    "    data['title_NN_count'] = data.title_postags.map(lambda x:x[0])\n",
    "    data['title_VERB_count'] = data.title_postags.map(lambda x:x[1])\n",
    "    data['title_ADJ_count'] = data.title_postags.map(lambda x:x[2])\n",
    "    # text\n",
    "    data['text_postags'] = data.texttoken_without_stopwords.apply(lambda x:count_postags(x))\n",
    "    data['text_NN_count'] = data.text_postags.map(lambda x:x[0])\n",
    "    data['text_VERB_count'] = data.text_postags.map(lambda x:x[1])\n",
    "    data['text_ADJ_count'] = data.text_postags.map(lambda x:x[2])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # create new features describe the length of the title and text, counting by words\n",
    "    # also maybe try with counting by letters\n",
    "    # and shall treat them as categorical variables group up with factor levels \n",
    "    data['title_length'] = data.title_token.apply(lambda x:len(x))\n",
    "    data['text_length'] = data.text_token.apply(lambda x:len(x))\n",
    "    # also highlight the keywords maybe?\n",
    "    \n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = prepare_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitdata(data, cols):\n",
    "    \"\"\"\n",
    "    cols: a list of columns names to use\n",
    "    \"\"\"\n",
    "    features = data[cols]\n",
    "    target = data.label\n",
    "    # first split into (train+test) and for holdout\n",
    "    TrainingSetX, X_holdout, TrainingSetY, y_holdout = train_test_split(features, target, \n",
    "                                                    test_size=0.10, stratify=target, \n",
    "                                                    random_state=666)\n",
    "    # split (train+test) into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(TrainingSetX, TrainingSetY, \n",
    "                                                test_size=0.20, stratify=TrainingSetY, \n",
    "                                                random_state=666)\n",
    "    # train and test use for modeling, holdout use for validating the model\n",
    "    print(\"spliting the data......\")\n",
    "    print(\"shape of train set: \", X_train.shape)\n",
    "    print(\"shape of test set: \", X_test.shape)\n",
    "    print(\"shape of holdout set: \", X_holdout.shape)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, X_holdout, y_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliting the data......\n",
      "shape of train set:  (2879, 5)\n",
      "shape of test set:  (720, 5)\n",
      "shape of holdout set:  (400, 5)\n"
     ]
    }
   ],
   "source": [
    "cols2model = ['title_token', 'text_token', 'titletoken_without_stopwords', \n",
    "              'texttoken_without_stopwords', 'title_length']\n",
    "x1,y1,x2,y2,x3,y3 = splitdata(train_df, cols = cols2model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest classifier\n",
    "def model_rf(data,cols2model):\n",
    "    # split data\n",
    "    X_train, y_train, X_test, y_test, X_holdout, y_holdout = splitdata(train_df, cols = cols2model)\n",
    "    print('features for modeling :',X_train.columns.tolist())\n",
    "    # identify the categorical features and the numerical features\n",
    "    categorical_features = X_train.dtypes == 'category'\n",
    "    numerical_features = ~categorical_features\n",
    "    # pipeline\n",
    "    print('building up pipeline......')\n",
    "    estimators = [   \n",
    "                    ('ctf',ColumnTransformer([\n",
    "                                ('scale',StandardScaler(),\n",
    "                                         numerical_features),\n",
    "                                ('enc',OneHotEncoder(categories='auto',handle_unknown='ignore'),\n",
    "                                         categorical_features),\n",
    "                                            ])\n",
    "                    ),\n",
    "#                     ('poly', PolynomialFeatures(degree=1, include_bias=False, interaction_only=True)),\n",
    "                    ('RF',RandomForestClassifier(random_state=666,n_estimators=170,max_depth=22,n_jobs=-1))\n",
    "                ]\n",
    "    pipe = Pipeline(estimators)\n",
    "    \n",
    "    print('training the model......')\n",
    "    model = pipe.fit(X_train,y_train)\n",
    "    \n",
    "    print('training is done')\n",
    "    score_train = model.score(X_train,y_train) \n",
    "    score_test = model.score(X_test,y_test) \n",
    "    score_Holdout = model.score(X_holdout,y_holdout) \n",
    "    \n",
    "    print('training dataset mode1 is: %s'%(str(score_train1)))   \n",
    "    print('test dataset model is: %s'%(str(score_test1)))\n",
    "    print('Holdout dataset model1 is: %s'%(str(score_H1)))\n",
    "    \n",
    "    # in case needs to use the data to do some visualization or some other metrics\n",
    "    data_pack = (X_train, y_train, X_test, y_test, X_holdout, y_holdout)\n",
    "    return model, data_pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
