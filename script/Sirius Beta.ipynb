{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk import word_tokenize, WordPunctTokenizer, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "import re\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = pd.read_csv('https://gist.githubusercontent.com/ScottPanIE/4dd8b7b85b44c18baf556d95b9093bdc/raw/88c4bfc393fa1fa004c0ceaceeedc6f84448f8be/fake_or_real_news_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../data/fake_or_real_news_training.csv')\n",
    "# # the submission data has no label\n",
    "# submission = pd.read_csv('../data/fake_or_real_news_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange(df):\n",
    "    for row in range(len(df)):\n",
    "        if not pd.isna(df.iloc[row, 5]):\n",
    "            # Concatenate according columns\n",
    "            df.iloc[row, 1] = df.iloc[row, 1] + df.iloc[row, 2] + df.iloc[row, 3]\n",
    "            df.iloc[row, 2] = df.iloc[row, 4]\n",
    "            df.iloc[row, 3] = df.iloc[row, 5]\n",
    "        elif not pd.isna(df.iloc[row, 4]):\n",
    "            df.iloc[row, 1] = df.iloc[row, 1] + df.iloc[row, 2]\n",
    "            df.iloc[row, 2] = df.iloc[row, 3]\n",
    "            df.iloc[row, 3] = df.iloc[row, 4]\n",
    "    df = df.drop(['X1', 'X2'], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter=PorterStemmer()\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence.lower())\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemma(text):    \n",
    "    tagged_sentence = pos_tag(word_tokenize(text))\n",
    "    lemm = [lemmatizer.lemmatize(word, pos=penn_to_wn(tag)) for word, tag in tagged_sentence if penn_to_wn(tag) != None ]\n",
    "    return \" \".join(lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    \"\"\"\n",
    "    processing data, on text and title\n",
    "    \"\"\"\n",
    "    # fix X1 X2 issue\n",
    "    data = rearrange(data)\n",
    "    # tokenize the title and text\n",
    "#     data['title'] = data.title.apply(lambda x: \" \".join(word_tokenize(x.lower())))\n",
    "#     data['text'] = data.text.apply(lambda x: \" \".join(word_tokenize(x.lower())))\n",
    "#     data['text_lemm'] = data.text.apply(lambda x: \" \".join(word_tokenize(x.lower())))\n",
    "    data['text_lemm'] = data.text.apply(lambda x: lemma(x))\n",
    "    data['text_lemm'] = data.text_lemm.apply(lambda x: re.sub(\"\\d+\", \"\", str(x)))\n",
    "\n",
    "    # add title into text\n",
    "    data['title_hash'] = data.title.apply(lambda x: \" \".join([\"TITLE_\"+w for w in word_tokenize(x)]))\n",
    "#     # concatenate two columns\n",
    "    data['whole'] = data['title_hash'] + data['text_lemm']\n",
    "#     data = data.drop(['title','text'], axis=1)\n",
    "    \n",
    "#     data['whole'] = data.whole.apply(lambda x: stemSentence(x))\n",
    "#     # remove numbers from the text\n",
    "#     data['whole'] = data.whole.apply(lambda x: re.sub(\"\\d+\", \"\", x))\n",
    "    # convert the target variable to 0 and 1\n",
    "    data.label = data.label.apply(lambda x: 1 if x == 'REAL' else 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload data\n",
    "train_0 = pd.read_csv('../data/fake_or_real_news_training.csv')\n",
    "train_1 = prepare_data(train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "def split_data(data, feature='whole'):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[feature], data['label'],\n",
    "                                                        test_size = 0.2, random_state = 7)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize\n",
    "def vectorize_select(selection=\"tfidf\", max_df=0.8, min_df=1):\n",
    "    \"\"\"\n",
    "    \"tfidf\":\"TfidVectorizer\"\n",
    "    \"count\":\"CountVectorizer\"\n",
    "    \"hash\":\"HashingVectorizer\"\n",
    "    \"\"\"\n",
    "    if selection == \"tfidf\":\n",
    "        return TfidfVectorizer(stop_words='english', max_df=max_df, min_df=min_df,binary=True,\n",
    "                              lowercase=False)\n",
    "    elif selection == \"count\":\n",
    "        return CountVectorizer(stop_words='english', max_df=max_df, min_df=min_df)\n",
    "    elif selection == \"hash\":\n",
    "        return HashingVectorizer(stop_words='english')\n",
    "    else:\n",
    "        raise Exception(\"{} can't be found\".format(selection))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PAC(data, selection='tfidf', vectorize_max_df=0.8, vectorize_min_df=1, feature='whole'):\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = split_data(data, feature=feature)\n",
    "    # vectorizer: selection: 'tfidf','count','hash'\n",
    "    vectorizer = vectorize_select(selection, max_df=vectorize_max_df, min_df=vectorize_min_df)\n",
    "    # transform data\n",
    "    vectorize_train = vectorizer.fit_transform(X_train)\n",
    "    vectorize_test = vectorizer.transform(X_test)\n",
    "    # model \n",
    "    linear_clf = PassiveAggressiveClassifier(random_state=666, max_iter=100, n_iter_no_change=10,\n",
    "                                             tol=1e-3,\n",
    "                                             early_stopping=True, validation_fraction=0.1)\n",
    "    linear_clf.fit(vectorize_train, y_train)\n",
    "    pred = linear_clf.predict(vectorize_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % acc)\n",
    "    print(confusion_matrix(y_test, pred, labels=[0, 1]))\n",
    "    \n",
    "    return linear_clf, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.954\n",
      "[[383  22]\n",
      " [ 15 380]]\n"
     ]
    }
   ],
   "source": [
    "m = PAC(train_1,feature='text')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
