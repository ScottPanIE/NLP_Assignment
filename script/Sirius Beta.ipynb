{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk import word_tokenize, WordPunctTokenizer, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "import re\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = pd.read_csv('https://gist.githubusercontent.com/ScottPanIE/4dd8b7b85b44c18baf556d95b9093bdc/raw/88c4bfc393fa1fa004c0ceaceeedc6f84448f8be/fake_or_real_news_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../data/fake_or_real_news_training.csv')\n",
    "# # the submission data has no label\n",
    "# submission = pd.read_csv('../data/fake_or_real_news_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange(df):\n",
    "    for row in range(len(df)):\n",
    "        if not pd.isna(df.iloc[row, 5]):\n",
    "            # Concatenate according columns\n",
    "            df.iloc[row, 1] = df.iloc[row, 1] + df.iloc[row, 2] + df.iloc[row, 3]\n",
    "            df.iloc[row, 2] = df.iloc[row, 4]\n",
    "            df.iloc[row, 3] = df.iloc[row, 5]\n",
    "        elif not pd.isna(df.iloc[row, 4]):\n",
    "            df.iloc[row, 1] = df.iloc[row, 1] + df.iloc[row, 2]\n",
    "            df.iloc[row, 2] = df.iloc[row, 3]\n",
    "            df.iloc[row, 3] = df.iloc[row, 4]\n",
    "    df = df.drop(['X1', 'X2'], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter=PorterStemmer()\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence.lower())\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemma(text):    \n",
    "    tagged_sentence = pos_tag(word_tokenize(text))\n",
    "    lemm = [lemmatizer.lemmatize(word, pos=penn_to_wn(tag)) for word, tag in tagged_sentence if penn_to_wn(tag) != None ]\n",
    "    return \" \".join(lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    \"\"\"\n",
    "    processing data, on text and title\n",
    "    \"\"\"\n",
    "    # fix X1 X2 issue\n",
    "    data = rearrange(data)\n",
    "    # tokenize the title and text\n",
    "#     data['title'] = data.title.apply(lambda x: \" \".join(word_tokenize(x.lower())))\n",
    "#     data['text'] = data.text.apply(lambda x: \" \".join(word_tokenize(x.lower())))\n",
    "#     data['text_lemm'] = data.text.apply(lambda x: \" \".join(word_tokenize(x.lower())))\n",
    "    data\n",
    "\n",
    "#     data['text_lemm'] = data.text.apply(lambda x: lemma(x))\n",
    "#     data['text_lemm'] = data.text_lemm.apply(lambda x: re.sub(\"\\d+\", \"\", str(x)))\n",
    "\n",
    "#     # add title into text\n",
    "#     data['title_hash'] = data.title.apply(lambda x: \" \".join([\"TITLE_\"+w for w in word_tokenize(x)]))\n",
    "# #     # concatenate two columns\n",
    "#     data['whole'] = data['title_hash'] + data['text_lemm']\n",
    "# #     data = data.drop(['title','text'], axis=1)\n",
    "    \n",
    "    \n",
    "#     data['whole'] = data.whole.apply(lambda x: stemSentence(x))\n",
    "#     # remove numbers from the text\n",
    "#     data['whole'] = data.whole.apply(lambda x: re.sub(\"\\d+\", \"\", x))\n",
    "    # convert the target variable to 0 and 1\n",
    "    data.label = data.label.apply(lambda x: 1 if x == 'REAL' else 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload data\n",
    "train_0 = pd.read_csv('../data/fake_or_real_news_training.csv')\n",
    "train_1 = prepare_data(train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0   8476                       You Can Smell Hillary’s Fear   \n",
       "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text  label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...      0  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...      0  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...      1  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...      0  \n",
       "4  It's primary day in New York and front-runners...      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "def split_data(data, feature='whole', random_state = 7):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[feature], data['label'],\n",
    "                                                        test_size = 0.2, random_state = random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize\n",
    "def vectorize_select(selection=\"tfidf\", max_df=0.8, min_df=1):\n",
    "    \"\"\"\n",
    "    \"tfidf\":\"TfidVectorizer\"\n",
    "    \"count\":\"CountVectorizer\"\n",
    "    \"hash\":\"HashingVectorizer\"\n",
    "    \"\"\"\n",
    "    if selection == \"tfidf\":\n",
    "        return TfidfVectorizer(stop_words='english', max_df=max_df, min_df=min_df,binary=True,\n",
    "                              lowercase=False)\n",
    "    elif selection == \"count\":\n",
    "        return CountVectorizer(stop_words='english', max_df=max_df, min_df=min_df)\n",
    "    elif selection == \"hash\":\n",
    "        return HashingVectorizer(stop_words='english')\n",
    "    else:\n",
    "        raise Exception(\"{} can't be found\".format(selection))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PAC(data, selection='tfidf', vectorize_max_df=0.8, vectorize_min_df=1, feature='whole'):\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = split_data(data, feature=feature)\n",
    "    # vectorizer: selection: 'tfidf','count','hash'\n",
    "    vectorizer = vectorize_select(selection, max_df=vectorize_max_df, min_df=vectorize_min_df)\n",
    "    # transform data\n",
    "    vectorize_train = vectorizer.fit_transform(X_train)\n",
    "    vectorize_test = vectorizer.transform(X_test)\n",
    "    # model \n",
    "    linear_clf = PassiveAggressiveClassifier(random_state=666, max_iter=100, n_iter_no_change=10,\n",
    "                                             tol=1e-3,\n",
    "                                             early_stopping=True, validation_fraction=0.1)\n",
    "    linear_clf.fit(vectorize_train, y_train)\n",
    "    pred = linear_clf.predict(vectorize_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % acc)\n",
    "    print(confusion_matrix(y_test, pred, labels=[0, 1]))\n",
    "    \n",
    "    return linear_clf, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.954\n",
      "[[383  22]\n",
      " [ 15 380]]\n"
     ]
    }
   ],
   "source": [
    "m = PAC(train_1,feature='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passive aggressive classifier with grid search cv\n",
    "def PAC_T(data, selection='tfidf', feature='text'):\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = split_data(data, feature=feature, random_state=666)\n",
    "    estimators = [   \n",
    "                    ('TF',TfidfVectorizer(stop_words='english', binary=True, lowercase=False,\n",
    "                                          max_df=0.8,\n",
    "                                         norm='l2',sublinear_tf=False)\n",
    "                    ),\n",
    "                    ('PA',PassiveAggressiveClassifier(random_state=666, max_iter=1000, \n",
    "                                                      n_iter_no_change=10, tol=1e-3, \n",
    "                                                      early_stopping=True, loss='squared_hinge',\n",
    "                                                      fit_intercept= False,\n",
    "                                                      validation_fraction=0.1))\n",
    "                ]\n",
    "    pipe = Pipeline(estimators)\n",
    "    param_RF = {\n",
    "                'TF__min_df':[4],\n",
    "                'TF__ngram_range': [(1,4)]\n",
    "                }\n",
    "    \n",
    "    # apply the estimators and parameters in pipeline\n",
    "    gridPipe = GridSearchCV(pipe, param_RF, cv=5, return_train_score=True)\n",
    "    model = gridPipe.fit(X_train, y_train)\n",
    "\n",
    "    pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % acc)\n",
    "    print(confusion_matrix(y_test, pred, labels=[0, 1]))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.951\n",
      "[[379  20]\n",
      " [ 19 382]]\n"
     ]
    }
   ],
   "source": [
    "m2 = PAC_T(prepare_dataFFF(train_0),feature='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('TF', TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=0.8, max_features=None, min_df=4,\n",
       "        ngram_range=(1, 4), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...      shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/intro_python/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/anaconda3/envs/intro_python/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.950\n",
      "[[1093   71]\n",
      " [  45 1112]]\n"
     ]
    }
   ],
   "source": [
    "# Slight modifications to the preparation-function\n",
    "# because the submission data has a slightly different structure\n",
    "submission = pd.read_csv('../data/fake_or_real_news_test.csv')\n",
    "# m2 is the model from the PAC function\n",
    "submission = prepare_dataFFF(submission)\n",
    "submission['prediction'] = m2.predict(submission['text'])\n",
    "submission.prediction[submission.prediction==0] = 'FAKE'\n",
    "submission.prediction[submission.prediction==1] = 'REAL'\n",
    "submission = submission.set_index('ID')\n",
    "\n",
    "# answer is the dataset from kaggle\n",
    "# this is the dataset downloaded from kaggle so I didn't put it inside the repo\n",
    "answer = pd.read_csv('../../fake_or_real_news.csv')\n",
    "answer.rename(columns={'Unnamed: 0':'ID'}, inplace=True)\n",
    "answer = answer.set_index('ID')\n",
    "\n",
    "# left join on submission with prediction\n",
    "submission2 = submission.join(answer[['label']],how='left')\n",
    "\n",
    "# metrics\n",
    "acc = accuracy_score(submission2.label, submission2.prediction)\n",
    "print(\"accuracy:   %0.3f\" % acc)\n",
    "print(confusion_matrix(submission2.label, submission2.prediction, labels=['FAKE', 'REAL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_date(x):\n",
    "    datelist = re.findall(r'\\w+\\s\\d{1,2},\\s\\d{4}', str(x))\n",
    "    if len(datelist) > 0:\n",
    "        dateinfo = \" HAS_DATE\"\n",
    "    else:\n",
    "        dateinfo = \" NO_DATE\"\n",
    "    return x + dateinfo\n",
    "\n",
    "def prepare_dataFFF(data):\n",
    "    \"\"\"\n",
    "    processing data, on text and title\n",
    "    \"\"\"\n",
    "    # fix X1 X2 issue\n",
    "    if 'X1' in data.columns:\n",
    "        data = rearrange(data)\n",
    "    # tokenize the title and text\n",
    "#     data['title'] = data.title.apply(lambda x: \" \".join(word_tokenize(x.lower())))\n",
    "#     data['text'] = data.text.apply(lambda x: \" \".join(word_tokenize(x.lower())))\n",
    "#     data['text_lemm'] = data.text.apply(lambda x: \" \".join(word_tokenize(x.lower())))\n",
    "    data['text_edit'] = data.text.apply(lambda x: re.sub(\"\\\\n\", \"\", str(x)))\n",
    "    data['text_edit'] = data.text_edit.apply(lambda x: count_date(x))\n",
    "\n",
    "#     data['text_lemm'] = data.text.apply(lambda x: lemma(x))\n",
    "#     data['text_lemm'] = data.text_lemm.apply(lambda x: re.sub(\"\\d+\", \"\", str(x)))\n",
    "\n",
    "#     # add title into text\n",
    "#     data['title_hash'] = data.title.apply(lambda x: \" \".join([\"TITLE_\"+w for w in word_tokenize(x)]))\n",
    "# #     # concatenate two columns\n",
    "#     data['whole'] = data['title_hash'] + data['text_lemm']\n",
    "# #     data = data.drop(['title','text'], axis=1)\n",
    "    \n",
    "    \n",
    "#     data['whole'] = data.whole.apply(lambda x: stemSentence(x))\n",
    "#     # remove numbers from the text\n",
    "#     data['whole'] = data.whole.apply(lambda x: re.sub(\"\\d+\", \"\", x))\n",
    "    # convert the target variable to 0 and 1\n",
    "    if 'label' in data.columns:\n",
    "        data.label = data.label.apply(lambda x: 1 if x == 'REAL' else 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
